{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-20T14:20:59.595280700Z",
     "start_time": "2024-05-20T14:20:44.152631600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from birdsong_dataset import AudioDataset , SoundData\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn, optim\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/birdsong_metadata.csv\")\n",
    "df.nunique()\n",
    "device = \"cuda\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T15:01:08.203825300Z",
     "start_time": "2024-05-20T15:01:08.159119400Z"
    }
   },
   "id": "f83f194f40a936d8",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "class transpose_layers(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # print(sample.shape)\n",
    "        \n",
    "        transposed = sample.transpose((2, 0, 1))\n",
    "        return transposed\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # print(sample.shape)\n",
    "        \n",
    "        \n",
    "        return torch.tensor(sample[0]).to(device) , torch.tensor(sample[1]).to(device) , torch.tensor(sample[2]).to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T15:07:22.736988100Z",
     "start_time": "2024-05-20T15:07:22.718889200Z"
    }
   },
   "id": "fa42b2e5f18218a1",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading The Data...\n"
     ]
    }
   ],
   "source": [
    "sounds = SoundData(df , transform = transpose_layers(), max_len= 4 , overlapping = 0 , noise_reduction=False, noise_reduction_level=0.6)\n",
    "dataset = AudioDataset(sounds.train , transform = ToTensor())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-20T15:44:19.265691900Z"
    }
   },
   "id": "e88c6582f2915956",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "# counter = 1\n",
    "# for i in range(1):\n",
    "#     for j in range(1):\n",
    "#         plt.subplot(1,1,counter)\n",
    "#         counter+=1\n",
    "#         plt.title(train[i][1].item())\n",
    "#         if j % 2 == 0 :\n",
    "#             plt.imshow(train[i][0].to('cpu').numpy().transpose((1,2,0)))\n",
    "#         else:\n",
    "#             plt.imshow(train[i][0].to('cpu').numpy().transpose((1,2,0)))\n",
    "        \n",
    "             "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:38:42.928427600Z",
     "start_time": "2024-05-15T12:38:42.906428Z"
    }
   },
   "id": "118c53e54622fc53",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# plt.figure(figsize=(10,10))\n",
    "# for i in range(8):\n",
    "#     plt.subplot(2,4,i+1)\n",
    "#     plt.title(train[i][1].item())\n",
    "#     plt.imshow(train[i][0].to('cpu').numpy().transpose((1,2,0)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:38:43.370348700Z",
     "start_time": "2024-05-15T12:38:43.354346400Z"
    }
   },
   "id": "e7c5729d47a50b5d",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "BirdSongClassifier(\n  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu1): ReLU()\n  (pool1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n  (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu2): ReLU()\n  (pool2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n  (norm3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (names_l1): Linear(in_features=1408, out_features=512, bias=True)\n  (names_relu1): ReLU()\n  (names_l2): Linear(in_features=512, out_features=88, bias=True)\n)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class BirdSongClassifier(nn.Module):\n",
    "    def __init__(self, num_classes_1 = 88 , num_classes_2 = 85):\n",
    "        super(BirdSongClassifier,self).__init__()\n",
    "        # backbone\n",
    "        self.conv1 = nn.Conv2d( in_channels=1, out_channels= 64 , kernel_size=3)\n",
    "        self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d((3,3))\n",
    "        self.conv2 = nn.Conv2d( in_channels=64, out_channels=128 , kernel_size=3)\n",
    "        self.norm2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d((3,3))\n",
    "        self.conv3 = nn.Conv2d( in_channels=128, out_channels=64 , kernel_size=3)\n",
    "        self.norm3 = nn.BatchNorm2d(64)\n",
    "        # self.relu3 = nn.ReLU()\n",
    "        # self.pool3 = nn.MaxPool2d((3,3))\n",
    "        \n",
    "        # multilabel classification for name \n",
    "        \n",
    "        self.names_l1 = nn.Linear(64*11*2 ,512 )\n",
    "        self.names_relu1 = nn.ReLU()\n",
    "        self.names_l2 = nn.Linear(512 , num_classes_1 )\n",
    "        \n",
    "        # multilabel classification for species       \n",
    "        \n",
    "        # self.species_l1 = nn.Linear(64*11*2 ,512 )\n",
    "        # self.species_relu1 = nn.ReLU()\n",
    "        # self.species_l2 = nn.Linear(512 , num_classes_2 )\n",
    "        # self.to(device)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        backbone = self.norm3(x)\n",
    "        # x = self.relu3(x)\n",
    "        # backbone = self.pool3(x)\n",
    "        # print(\"Backbone Shape : \", backbone.size())\n",
    "        #forward for names\n",
    "        x1 = backbone.view(-1, 64*11*2)\n",
    "        x1 = self.names_l1(x1.reshape(-1,64*11*2))\n",
    "        x1 = self.names_relu1(x1)\n",
    "        x1 = self.names_l2(x1)\n",
    "        # print(\"Name Head Shape = \" ,x1.shape)\n",
    "\n",
    "        #forward for species\n",
    "\n",
    "        # x2 = backbone.view(-1, 64*11*2)\n",
    "        # x2 = self.species_l1(x2.reshape(-1,64*11*2))\n",
    "        # x2 = self.species_relu1(x2)\n",
    "        # x2 = self.species_l2(x2)\n",
    "        return x1 \n",
    "\n",
    "    def test_model(self, data_loader):\n",
    "        \"\"\"\n",
    "        Test a model on a specified dataset.\n",
    "    \n",
    "        Inputs:\n",
    "            net - Trained model of type BaseNetwork\n",
    "            data_loader - DataLoader object of the dataset to test on (validation or test)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        true_preds_names, true_preds_species,  count = 0., 0., 0\n",
    "        \n",
    "        for imgs, names , species in data_loader: \n",
    "            imgs, names , species = imgs.to(device), names.to(device) , species.to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = self(imgs)\n",
    "                preds_names = preds[0].argmax(dim=-1)\n",
    "                # preds_species = preds[1].argmax(dim=-1)\n",
    "                true_preds_names += (preds_names == names )[0].sum().item()\n",
    "                # true_preds_species += (preds_species == species )[0].sum().item()\n",
    "                count += 2\n",
    "        test_acc = (true_preds_names +  true_preds_species) / count\n",
    "        return test_acc\n",
    "model = BirdSongClassifier()\n",
    "model.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T15:15:39.064973300Z",
     "start_time": "2024-05-20T15:15:39.016511800Z"
    }
   },
   "id": "3e95fa90b7d66bff",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.45151515151515154"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the sizes of your desired splits\n",
    "train_size = int(0.6 * len(dataset))  # 60% of the data for training\n",
    "val_size = int(0.25 * len(dataset))   # 25% of the data for validation\n",
    "test_size = len(dataset) - train_size - val_size  # Remaining data for testing\n",
    "\n",
    "# Use random_split with shuffled indices to create the splits\n",
    "train, val, test = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 2\n",
    "# You can then create data loaders for each split if needed\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.test_model(test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T15:35:18.351240500Z",
     "start_time": "2024-05-20T15:35:15.231115900Z"
    }
   },
   "id": "5486d021743b429a",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss : 0.254268 , Train Acc : 0.4751, Val Acc : 0.44611187 , Best Acc : 0.00000000\n",
      "Epoch [2/50], Training Loss : 0.152521 , Train Acc : 0.4883, Val Acc : 0.45975443 , Best Acc : 0.00000000\n",
      "Epoch [3/50], Training Loss : 0.118688 , Train Acc : 0.4779, Val Acc : 0.44884038 , Best Acc : 0.00000000\n",
      "Epoch [4/50], Training Loss : 0.074432 , Train Acc : 0.4809, Val Acc : 0.45247840 , Best Acc : 0.00000000\n",
      "Epoch [5/50], Training Loss : 0.053257 , Train Acc : 0.4899, Val Acc : 0.46066394 , Best Acc : 0.00000000\n",
      "Epoch [6/50], Training Loss : 0.040441 , Train Acc : 0.4873, Val Acc : 0.46452933 , Best Acc : 0.00000000\n",
      "Epoch [7/50], Training Loss : 0.028402 , Train Acc : 0.4989, Val Acc : 0.47407913 , Best Acc : 0.00000000\n",
      "Epoch [8/50], Training Loss : 0.024509 , Train Acc : 0.4890, Val Acc : 0.46066394 , Best Acc : 0.00000000\n",
      "Epoch [9/50], Training Loss : 0.020099 , Train Acc : 0.4867, Val Acc : 0.45861755 , Best Acc : 0.00000000\n",
      "Epoch [10/50], Training Loss : 0.017519 , Train Acc : 0.4884, Val Acc : 0.46043656 , Best Acc : 0.00000000\n",
      "TEST ACCURACY : 0.4580\n",
      "Epoch [11/50], Training Loss : 0.015350 , Train Acc : 0.4910, Val Acc : 0.46930423 , Best Acc : 0.00000000\n",
      "Epoch [12/50], Training Loss : 0.013063 , Train Acc : 0.4999, Val Acc : 0.47430650 , Best Acc : 0.00000000\n",
      "Epoch [13/50], Training Loss : 0.013791 , Train Acc : 0.4873, Val Acc : 0.45884493 , Best Acc : 0.00000000\n",
      "Epoch [14/50], Training Loss : 0.012397 , Train Acc : 0.4856, Val Acc : 0.46248295 , Best Acc : 0.00000000\n",
      "Epoch [15/50], Training Loss : 0.010895 , Train Acc : 0.4925, Val Acc : 0.46589359 , Best Acc : 0.00000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[47], line 33\u001B[0m\n\u001B[0;32m     31\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m loss_names\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[1;32m---> 33\u001B[0m \u001B[43mtotal_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# t_loss_species.backward(retain_graph= True)\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# Update weights\u001B[39;00m\n\u001B[0;32m     36\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\BirdSongClassification\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\BirdSongClassification\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion_names = nn.CrossEntropyLoss()\n",
    "criterion_species = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001 , weight_decay=0.01, momentum=0.9)\n",
    "max_val_accuracy = 0 \n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels_names, labels_species) in enumerate(train_loader):\n",
    "        inputs , labels_names , _ = inputs , labels_names , labels_species\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs_names  = model(inputs.type(torch.cuda.FloatTensor))\n",
    "        # print(outputs_names.shape)\n",
    "        # print(labels_names.shape)\n",
    "        # print(torch.argmax(outputs_names , dim=-1).item(), labels_names.item() ,\"-------\", torch.argmax(outputs_species , dim=-1).item(), labels_species.item())\n",
    "        # time.sleep(0.1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss_names = criterion_names(outputs_names, labels_names.type(torch.cuda.LongTensor))\n",
    "        # loss_species = criterion_species(outputs_species, labels_species.type(torch.LongTensor).to(device))\n",
    "        t_loss_names = loss_names \n",
    "        # t_loss_species = loss_species\n",
    "        # print(t_loss_names , t_loss_species)\n",
    "        total_loss = loss_names\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        # t_loss_species.backward(retain_graph= True)\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        # print(total_loss)\n",
    "        # optimizer_species.step()\n",
    "        # print(total_loss)\n",
    "        running_loss += total_loss.item()\n",
    "        # running_loss_species += t_loss_species.item()\n",
    "        \n",
    "        # Print statistics every 100 batches\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        train_acc = model.test_model(train_loader) \n",
    "        val_acc = model.test_model(val_loader)\n",
    "        if epoch > 0 and val_acc > 0.6 and val_acc > max_val_accuracy :\n",
    "            torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, f\"best.pt\")\n",
    "            max_val_accuracy = val_acc\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss : {batch_size * running_loss / len(train)  :.6f} , Train Acc : {train_acc:.4f}, Val Acc : {val_acc:.8f} , Best Acc : {max_val_accuracy:0.8f}')\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        test_acc = model.test_model(test_loader)\n",
    "        print(f\"TEST ACCURACY : {test_acc:0.4f}\")\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T15:44:12.781971Z",
     "start_time": "2024-05-20T15:35:21.281484Z"
    }
   },
   "id": "66ceadb4427a31cc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cbc7a0e01314b5f1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "BirdSongClassifier(\n  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu1): ReLU()\n  (pool1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n  (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu2): ReLU()\n  (pool2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n  (norm3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (names_l1): Linear(in_features=1408, out_features=512, bias=True)\n  (names_relu1): ReLU()\n  (names_l2): Linear(in_features=512, out_features=88, bias=True)\n  (species_l1): Linear(in_features=1408, out_features=512, bias=True)\n  (species_relu1): ReLU()\n  (species_l2): Linear(in_features=512, out_features=85, bias=True)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "# model_ft = BirdSongClassifier()\n",
    "# optimizer_ft = optim.SGD(model.parameters(), lr=0.00001)\n",
    "# \n",
    "# checkpoint = torch.load(\"best.pt\" , map_location= device)\n",
    "# model_ft.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer_ft.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# model_ft.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:40:44.805351500Z",
     "start_time": "2024-05-15T12:40:44.737081100Z"
    }
   },
   "id": "ff7b42498b442992",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 31\u001B[0m\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;66;03m# running_loss_species += t_loss_species.item()\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \n\u001B[0;32m     29\u001B[0m     \u001B[38;5;66;03m# Print statistics every 100 batches\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m---> 31\u001B[0m     train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_ft\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m \n\u001B[0;32m     32\u001B[0m     val_acc \u001B[38;5;241m=\u001B[39m model_ft\u001B[38;5;241m.\u001B[39mtest_model(val_loader)\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m], Training Loss : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrunning_loss\u001B[38;5;250m \u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.6f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m , Train Acc : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Val Acc : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.8f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[8], line 76\u001B[0m, in \u001B[0;36mBirdSongClassifier.test_model\u001B[1;34m(self, data_loader)\u001B[0m\n\u001B[0;32m     74\u001B[0m imgs, names , species \u001B[38;5;241m=\u001B[39m imgs\u001B[38;5;241m.\u001B[39mto(device), names\u001B[38;5;241m.\u001B[39mto(device) , species\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 76\u001B[0m     preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimgs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m     preds_names \u001B[38;5;241m=\u001B[39m preds[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     78\u001B[0m     preds_species \u001B[38;5;241m=\u001B[39m preds[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\BirdSongClassification_20240502\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[8], line 48\u001B[0m, in \u001B[0;36mBirdSongClassifier.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# x = self.relu3(x)\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# backbone = self.pool3(x)\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# print(\"Backbone Shape : \", backbone.size())\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m#forward for names\u001B[39;00m\n\u001B[0;32m     47\u001B[0m x1 \u001B[38;5;241m=\u001B[39m backbone\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m64\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m11\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m---> 48\u001B[0m x1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnames_l1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m11\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m x1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnames_relu1(x1)\n\u001B[0;32m     50\u001B[0m x1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnames_l2(x1)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\BirdSongClassification_20240502\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\BirdSongClassification_20240502\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# num_epochs = 1000\n",
    "# for epoch in range(num_epochs):\n",
    "#     model_ft.train()\n",
    "#     running_loss = 0.0\n",
    "#     for i, (inputs, labels_names, labels_species) in enumerate(train_loader):\n",
    "# \n",
    "#         labels_names , labels_species = labels_names.to(\"cuda\") , labels_species.to(\"cuda\")\n",
    "#         optimizer.zero_grad()\n",
    "#         # Forward pass\n",
    "#         outputs_names , outputs_species = model_ft(inputs.type(torch.cuda.FloatTensor))\n",
    "#         # print(torch.argmax(outputs_names , dim=-1).item(), labels_names.item() ,\"-------\", torch.argmax(outputs_species , dim=-1).item(), labels_species.item())\n",
    "#         # time.sleep(0.1)\n",
    "# \n",
    "#         # Calculate loss\n",
    "#         loss_names = criterion_names(outputs_names, labels_names.type(torch.cuda.LongTensor) )\n",
    "#         loss_species = criterion_species(outputs_species, labels_species.type(torch.cuda.LongTensor))\n",
    "#         t_loss_names = loss_names \n",
    "#         t_loss_species = loss_species\n",
    "#         total_loss = loss_names + loss_species\n",
    "#         # Backward pass\n",
    "#         total_loss.backward()\n",
    "#         # t_loss_species.backward(retain_graph= True)\n",
    "#         # Update weights\n",
    "#         optimizer.step()\n",
    "#         # optimizer_species.step()\n",
    "#         running_loss += total_loss.item()\n",
    "#         # running_loss_species += t_loss_species.item()\n",
    "# \n",
    "#         # Print statistics every 100 batches\n",
    "#     if (epoch + 1) % 1 == 0:\n",
    "#         train_acc = model_ft.test_model(train_loader) \n",
    "#         val_acc = model_ft.test_model(val_loader)\n",
    "# \n",
    "#         print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss : {running_loss :.6f} , Train Acc : {train_acc:.4f}, Val Acc : {val_acc:.8f}')\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T12:41:25.119232700Z",
     "start_time": "2024-05-15T12:41:02.727386700Z"
    }
   },
   "id": "55f36732b6457773"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input = torch.randn(1, 5, requires_grad=True)\n",
    "input.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-14T14:17:41.339114200Z"
    }
   },
   "id": "4068e632cb43ac38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = torch.empty(1, dtype=torch.long).random_(5)\n",
    "target.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-14T14:17:41.340111800Z"
    }
   },
   "id": "51193654b841bc3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "target_layers = [model.layer4[-1]]\n",
    "input_tensor = # Create an input tensor image for your model..\n",
    "# Note: input_tensor can be a batch tensor with several images!\n",
    "\n",
    "# Construct the CAM object once, and then re-use it on many images:\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "\n",
    "# You can also use it within a with statement, to make sure it is freed,\n",
    "# In case you need to re-create it inside an outer loop:\n",
    "# with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "#   ...\n",
    "\n",
    "# We have to specify the target we want to generate\n",
    "# the Class Activation Maps for.\n",
    "# If targets is None, the highest scoring category\n",
    "# will be used for every image in the batch.\n",
    "# Here we use ClassifierOutputTarget, but you can define your own custom targets\n",
    "# That are, for example, combinations of categories, or specific outputs in a non standard model.\n",
    "\n",
    "targets = [ClassifierOutputTarget(281)]\n",
    "\n",
    "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "# In this example grayscale_cam has only one image in the batch:\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "# You can also get the model outputs without having to re-inference\n",
    "model_outputs = cam.outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-14T14:17:41.341108600Z"
    }
   },
   "id": "ae66c2864256307b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
